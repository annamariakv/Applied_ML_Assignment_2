{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1205461",
   "metadata": {},
   "source": [
    "### Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29ff7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  \n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e6496",
   "metadata": {},
   "source": [
    "### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "validation = pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6439f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"text\"]\n",
    "X_test = test[\"text\"]\n",
    "X_val = validation[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f51136",
   "metadata": {},
   "source": [
    "### Vectorizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44a664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text):\n",
    "    tokens = CountVectorizer().build_tokenizer()(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d37966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenizer_)\n",
    "\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0620c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/24 00:25:21 INFO mlflow.tracking.fluent: Experiment with name 'Assignment_2' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///D:/Cmi/Applied%20ML/Assignment_2/mlruns/325378683522626974', creation_time=1708714521064, experiment_id='325378683522626974', last_update_time=1708714521064, lifecycle_stage='active', name='Assignment_2', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Assignment_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37307c",
   "metadata": {},
   "source": [
    "### Naive Bayes Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd7f3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation AUC-PR: 0.9833177961924675\n",
      "Naive Bayes Test AUC-PR: 0.9842469896318956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Naive Bayes'.\n",
      "Created version '1' of model 'Naive Bayes'.\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "model_name = \"Naive Bayes\"\n",
    "with mlflow.start_run(run_name=model_name):\n",
    "    # Build and train the model\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    val_predictions = classifier.predict(X_val_vectorized)\n",
    "    \n",
    "    # Calculate precision-recall curve for validation set\n",
    "    precision, recall, _ = precision_recall_curve(y_val, val_predictions)\n",
    "\n",
    "    # Calculate AUC-PR for validation set\n",
    "    auc_pr_val = auc(recall, precision)\n",
    "\n",
    "    # Log parameters and metrics for validation set\n",
    "    mlflow.log_params(classifier.get_params())\n",
    "    mlflow.log_metric(\"AUCPR_val\", auc_pr_val)\n",
    "    mlflow.sklearn.log_model(classifier, \"model\")\n",
    "\n",
    "    print(f\"{model_name} Validation AUC-PR: {auc_pr_val}\")\n",
    "\n",
    "    # Predict probabilities on the test set \n",
    "    test_predictions = classifier.predict(X_test_vectorized)\n",
    "\n",
    "    # Calculate precision-recall curve for test set\n",
    "    precision_test, recall_test, _ = precision_recall_curve(y_test, test_predictions)\n",
    "\n",
    "    # Calculate AUC-PR for test set\n",
    "    auc_pr_test = auc(recall_test, precision_test)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"AUCPR_test\", auc_pr_test)\n",
    "\n",
    "    print(f\"{model_name} Test AUC-PR: {auc_pr_test}\")\n",
    "\n",
    "    # Register the model\n",
    "    mlflow.register_model(mlflow.get_artifact_uri(\"model\"), \"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ec658",
   "metadata": {},
   "source": [
    "### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55045d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation AUC-PR: 0.9632421740848977\n",
      "SVM Test AUC-PR: 0.9632509728503631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'SVM'.\n",
      "Created version '1' of model 'SVM'.\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "model_name_svm = \"SVM\"\n",
    "with mlflow.start_run(run_name=model_name_svm):\n",
    "    # Build and train the SVM model\n",
    "    svm_classifier = SVC(probability=True)  \n",
    "    svm_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    val_predictions_svm = svm_classifier.predict(X_val_vectorized)\n",
    "\n",
    "    # Calculate precision-recall curve for validation set\n",
    "    precision_svm, recall_svm, _ = precision_recall_curve(y_val, val_predictions_svm)\n",
    "\n",
    "    # Calculate AUC-PR for validation set\n",
    "    auc_pr_val_svm = auc(recall_svm, precision_svm)\n",
    "\n",
    "    # Log parameters and metrics for validation set\n",
    "    mlflow.log_params(svm_classifier.get_params())\n",
    "    mlflow.log_metric(\"AUCPR_val\", auc_pr_val_svm)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"model\")\n",
    "\n",
    "    print(f\"{model_name_svm} Validation AUC-PR: {auc_pr_val_svm}\")\n",
    "\n",
    "    # Predict probabilities on the test set\n",
    "    test_predictions_svm = svm_classifier.predict(X_test_vectorized)\n",
    "\n",
    "    # Calculate precision-recall curve for test set\n",
    "    precision_test_svm, recall_test_svm, _ = precision_recall_curve(y_test, test_predictions_svm)\n",
    "\n",
    "    # Calculate AUC-PR for test set\n",
    "    auc_pr_test_svm = auc(recall_test_svm, precision_test_svm)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"AUCPR_test\", auc_pr_test_svm)\n",
    "\n",
    "    print(f\"{model_name_svm} Test AUC-PR: {auc_pr_test_svm}\")\n",
    "\n",
    "    # Register the model\n",
    "    mlflow.register_model(mlflow.get_artifact_uri(\"model\"), model_name_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564f624",
   "metadata": {},
   "source": [
    "### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae9ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Validation AUC-PR: 0.9740529848957085\n",
      "RandomForest Test AUC-PR: 0.9760570310718529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'RandomForest'.\n",
      "Created version '1' of model 'RandomForest'.\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "model_name_rf = \"RandomForest\"\n",
    "with mlflow.start_run(run_name=model_name_rf):\n",
    "    # Build and train the Random Forest model\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    rf_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_predictions_rf = rf_classifier.predict(X_val_vectorized)\n",
    "\n",
    "    # Calculate precision-recall curve for validation set\n",
    "    precision_rf, recall_rf, _ = precision_recall_curve(y_val, val_predictions_rf)\n",
    "\n",
    "    # Calculate AUC-PR for validation set\n",
    "    auc_pr_val_rf = auc(recall_rf, precision_rf)\n",
    "\n",
    "    # Log parameters and metrics for validation set\n",
    "    mlflow.log_params(rf_classifier.get_params())\n",
    "    mlflow.log_metric(\"AUCPR_val\", auc_pr_val_rf)\n",
    "    mlflow.sklearn.log_model(rf_classifier, \"model\")\n",
    "\n",
    "    print(f\"{model_name_rf} Validation AUC-PR: {auc_pr_val_rf}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    test_predictions_rf = rf_classifier.predict(X_test_vectorized)\n",
    "\n",
    "    # Calculate precision-recall curve for test set\n",
    "    precision_test_rf, recall_test_rf, _ = precision_recall_curve(y_test, test_predictions_rf)\n",
    "\n",
    "    # Calculate AUC-PR for test set\n",
    "    auc_pr_test_rf = auc(recall_test_rf, precision_test_rf)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"AUCPR_test\", auc_pr_test_rf)\n",
    "\n",
    "    print(f\"{model_name_rf} Test AUC-PR: {auc_pr_test_rf}\")\n",
    "\n",
    "    # Register the model\n",
    "    mlflow.register_model(mlflow.get_artifact_uri(\"model\"), model_name_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c853e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
